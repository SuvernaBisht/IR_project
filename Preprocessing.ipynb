{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvHZkFJnEPop"
      },
      "source": [
        "# Pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OduYR3WEOAB"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo8l5dHrEZQZ"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEVn8qlAEdj8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc2UFT8CEg0K"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import emoji\n",
        "\n",
        "path = '/content/gdrive/MyDrive/IR_Project/Project/data1000/labeled/negative/timeline/'\n",
        "\n",
        "from pathlib import Path\n",
        "folder = Path(path).rglob('*.json')\n",
        "files = [x for x in folder]\n",
        "\n",
        "def get_emojis(s):\n",
        "  orig_list = [s]\n",
        "  emojis_iter = map(lambda y: y, emoji.UNICODE_EMOJI['en'].keys())\n",
        "  regex_set = re.compile('|'.join(re.escape(em) for em in emojis_iter))\n",
        "  new_list = regex_set.findall(orig_list[0])\n",
        "  return new_list\n",
        "\n",
        "pos_wrds = ['happy','pretty','good','joy','love','pride','win','certainty']\n",
        "neg_wrds = ['hate','worthless','enemy','nervous','sad','afraid','tense','kill','grief','cry']\n",
        "first_sing = ['i','me','my']\n",
        "first_plu = [ 'we','u','our']\n",
        "\n",
        "def pos_wrd_cnt(txt):\n",
        "  count = 0\n",
        "  for wrd in pos_wrds:\n",
        "    patterns = ['^'+wrd,' '+ wrd+' ', wrd+'$']\n",
        "    for pattern in patterns:\n",
        "     count = count + len(re.findall(pattern, txt))\n",
        "  return count\n",
        "  \n",
        "\n",
        "def neg_wrd_cnt(txt):\n",
        "  count = 0\n",
        "  for wrd in neg_wrds:\n",
        "    patterns = ['^'+wrd,' '+ wrd+' ', wrd+'$']\n",
        "    for pattern in patterns:\n",
        "     count = count + len(re.findall(pattern, txt))\n",
        "  return count\n",
        "\n",
        "def first_sing_cnt(txt):\n",
        "  count = 0\n",
        "  for wrd in first_sing:\n",
        "    patterns = ['^'+wrd,' '+ wrd+' ', wrd+'$']\n",
        "    for pattern in patterns:\n",
        "     count = count + len(re.findall(pattern, txt))\n",
        "  return count\n",
        "\n",
        "def first_plu_cnt(txt):\n",
        "  count = 0\n",
        "  for wrd in first_plu:\n",
        "    patterns = ['^'+wrd,' '+ wrd+' ', wrd+'$']\n",
        "    for pattern in patterns:\n",
        "     count = count + len(re.findall(pattern, txt))\n",
        "  return count\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "def lwr_spell(txt):\n",
        "  ltxt = txt.lower()\n",
        "  textBlb = TextBlob(ltxt)            # Making our first textblob\n",
        "  textCorrected = textBlb.correct()   # Correcting the text\n",
        "  return textCorrected\n",
        "\n",
        "def tag_rem(txt,substr):\n",
        "  txt = txt.replace('@' + substr , ' ') \n",
        "  return txt\n",
        "\n",
        "def get_text(s):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', s)\n",
        "lemoji = []\n",
        "ltweets = []\n",
        "lemojinum = []\n",
        "lposemo = []\n",
        "lnegemo = []\n",
        "luserid = []\n",
        "lsing = []\n",
        "lplu = []\n",
        "\n",
        "for file in files:\n",
        "  data = pd.read_json(file,lines=True)\n",
        "  \n",
        "  if(data.empty):\n",
        "    print(\"ERROR\")\n",
        "    continue\n",
        "  date=data['created_at']\n",
        "  length = len(date)\n",
        "  text=data['text']\n",
        "  \n",
        "  user = data['user']\n",
        "  entities=data['entities']\n",
        "  \n",
        "  tottext=''\n",
        "  totemocnt = 0\n",
        "  totposcnt = 0 \n",
        "  totnegcnt = 0\n",
        "  totfps = 0\n",
        "  totfpp = 0\n",
        "  totemo = []\n",
        "  userid = user[0].get('id')\n",
        "  \n",
        "  \n",
        "  for i in range(length):\n",
        "    tagrem_txt = text[i].lower()\n",
        "    if(len(entities[i].get('user_mentions'))!=0):\n",
        "      en_user = entities[i].get('user_mentions')\n",
        "      en_len = len( en_user)\n",
        "      for k in range(en_len):\n",
        "        tag = en_user[k].get('screen_name')\n",
        "        tagrem_txt = tag_rem(tagrem_txt,tag.lower())\n",
        "\n",
        "    txt_s= re.sub(r'^u ', 'you ', tagrem_txt) \n",
        "    txt_m = re.sub(r' u ', ' you ', txt_s)\n",
        "    txt_e = re.sub(r' u$', ' you', txt_m)\n",
        "    emojis = get_emojis(text[i])\n",
        "    totemo = totemo + emojis\n",
        "    text_rt= re.sub(r'^rt ', '', txt_e)    \n",
        "    linkrem_txt= re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text_rt, flags=re.MULTILINE)\n",
        "    puncrem_txt = re.sub(r'[^\\w\\s]', '', linkrem_txt) \n",
        "\n",
        "    lem_txt = lemmatize_words(puncrem_txt)\n",
        "    tweet = get_text(lem_txt)\n",
        "    \n",
        "    tottext = tottext +tweet+'*'\n",
        "    if((i+1)%20==0):\n",
        "      lemoji.append(totemo)\n",
        "      lposemo.append(pos_wrd_cnt(tottext))\n",
        "      lnegemo.append(neg_wrd_cnt(tottext))\n",
        "      lemojinum.append(len(totemo))\n",
        "      lsing.append(first_sing_cnt(tottext))\n",
        "      lplu.append(first_plu_cnt(tottext))\n",
        "         \n",
        "      ltweets.append(tottext)\n",
        "      luserid.append(userid)\n",
        "      tottext=''\n",
        "      totemo = []\n",
        "\n",
        "  lemoji.append(totemo)\n",
        "  lposemo.append(pos_wrd_cnt(tottext))\n",
        "  lnegemo.append(neg_wrd_cnt(tottext))\n",
        "  lemojinum.append(len(totemo))\n",
        "  lsing.append(first_sing_cnt(tottext))\n",
        "  lplu.append(first_plu_cnt(tottext))\n",
        "  #print(tottext)\n",
        "  \n",
        "  ltweets.append(tottext)\n",
        "  luserid.append(userid)\n",
        "   \n",
        "\n",
        "dict = {'userid':luserid,'tweet': ltweets, 'emoji': lemoji, 'num_emoji': lemojinum,'+ve_emo_wrdcount':lposemo, \n",
        "          '-ve_emo_wrdcount': lnegemo, 'first_person_sing':lsing,'first_person_plu':lplu}  \n",
        "  \n",
        "df = pd.DataFrame(dict) \n",
        "df.to_csv(r'/content/gdrive/MyDrive/IR_Project/Project/data1000/labeled/negative/features_negative_20.csv',  index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ6XKIdFEPOT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}